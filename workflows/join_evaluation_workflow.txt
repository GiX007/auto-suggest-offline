JOIN COLUMN & JOIN TYPE EVALUATION WORKFLOW (join_eval)

PHASE 0: ENTRY POINT
---------------------
Function: run_join_prediction(..., mode='eval')
File: src/main.py
Purpose: Orchestrates evaluation pipeline for join column and join type models

PHASE 1: LOAD & PROCESS TEST SAMPLES
-------------------------------------
1. load_operator_samples(join_dir, 'join')
   - File: src/data/sample_loader.py
   - Loads all sample folders from the merge/ directory

2. load_sample(...)
   - File: src/data/sample_loader.py
   - Loads:
     * 'left_table' from left.csv
     * 'right_table' from right.csv
     * 'params' from param.json

3. process_join_samples(join_samples)
   - File: src/features/join_features.py
   - Extracts:
     * Join columns
     * Join type
     * Handles edge cases like 'index', 'Unnamed: 0'
   - Output: List of processed samples with join keys and type

4. Split test set:
   - 20% test split is used here separately from training-time split
   - Ensures clean generalization testing

PHASE 2: LOAD TRAINED MODELS
-----------------------------
5. load_model("join_column_model.pkl")
   - File: src/utils/model_utils.py
   - Returns: (model, feature_names)

6. load_model("join_type_model.pkl")
   - File: src/utils/model_utils.py
   - Returns: (model, feature_names, label_encoder)

PHASE 3: EVALUATE JOIN COLUMN MODEL 
------------------------------------
7. evaluate_join_column_model(model, feature_names, test_samples, k_values)
   - File: src/utils/evaluation.py
   - Calls:
     * predict_join_columns(...) → src/models/join_col_model.py
     * evaluate_per_sample_ranking(...) → src/utils/evaluation.py
     * generate_join_column_table(...) → src/utils/evaluation.py
     * generate_feature_importance_table(...) → src/utils/evaluation.py
   - Compares predictions to ground truth and computes:
     * precision@k
     * ndcg@k
   - Saves:
     * results/metrics/join_column_eval_metrics.csv
     * results/figures/join_column_feature_importance.png
     * Table 3: join_column_literature_comparison.csv
     * Table 4: printed to console (feature group importance)

PHASE 4: EVALUATE JOIN TYPE MODEL
----------------------------------
8. evaluate_join_type_model(model, feature_names, label_encoder, test_samples)
   - File: src/utils/evaluation.py
   - Calls:
     * predict_join_type(...) → src/models/join_type_model.py
     * generate_join_type_table(...) → src/utils/evaluation.py
   - Compares predicted join type vs. ground truth
   - Computes:
     * accuracy
     * confusion matrix
     * class-wise precision/recall (optional metrics_dict fields)
   - Saves:
     * results/metrics/join_type_eval_metrics.csv
     * results/figures/join_type_confusion_matrix.png
     * Table 5: printed to console (comparison with Vendor-A)

PHASE 5: EVALUATE BASELINE METHODS (JOIN COLUMN)
--------------------------------------------------
9. evaluate_baselines(test_samples, k_values)
   - File: src/baselines/join_baselines.py
   - Evaluates five literature methods:
     * ML-FK
     * PowerPivot
     * Multi
     * Holistic
     * max-overlap
   - Computes:
     * precision@k
     * ndcg@k
   - Output passed to:
     * generate_join_column_table(...) → src/utils/evaluation.py
     - Produces Table 3 (literature & vendor comparison) printed and saved


COMMAND TO RUN
---------------------
To execute the full evaluation pipeline, run the following:

    python -m src.main --operator join --mode eval

---------------------
